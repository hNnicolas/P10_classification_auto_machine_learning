{
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "# üìä HR Analytics ‚Äì Analyse et Pr√©paration des Donn√©es\n",
     "## Projet TechNova Partners\n",
     "\n",
     "Ce notebook pr√©sente l‚Äôensemble du travail r√©alis√© :\n",
     "- Nettoyage et pr√©paration des donn√©es RH (SIRH, √©valuations, sondages).\n",
     "- Analyse descriptive et exploration visuelle.\n",
     "- Construction d‚Äôun DataFrame central.\n",
     "- Tests de mod√©lisation pour la pr√©diction du d√©part des employ√©s.\n",
     "\n",
     "üéØ **Objectif final :** fournir au CODIR des analyses fiables et actionnables sur la r√©tention et la gestion des talents."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Import des librairies principales\n",
     "import pandas as pd\n",
     "import numpy as np\n",
     "import matplotlib.pyplot as plt\n",
     "import seaborn as sns\n",
     "\n",
     "from sklearn.model_selection import train_test_split\n",
     "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
     "from sklearn.impute import SimpleImputer\n",
     "from sklearn.dummy import DummyClassifier\n",
     "from sklearn.linear_model import LogisticRegression\n",
     "from sklearn.ensemble import RandomForestClassifier\n",
     "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
     "\n",
     "%matplotlib inline\n",
     "sns.set(style=\"whitegrid\")\n",
     "pd.set_option('display.max_columns', None)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 1Ô∏è‚É£ Chargement des donn√©es\n",
     "Nous disposons de 3 fichiers principaux :\n",
     "- **SIRH** : informations RH (√¢ge, revenu, anciennet√©, etc.)\n",
     "- **√âvaluations** : r√©sultats des entretiens annuels\n",
     "- **Sondages** : climat social et satisfaction des collaborateurs"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "sirh = pd.read_csv('data/extrait_sirh.csv')\n",
     "evals = pd.read_csv('data/extrait_eval.csv')\n",
     "sondage = pd.read_csv('data/extrait_sondage.csv')\n",
     "\n",
     "print('=== SIRH ===')\n",
     "display(sirh.head())\n",
     "print('=== Eval ===')\n",
     "display(evals.head())\n",
     "print('=== Sondage ===')\n",
     "display(sondage.head())"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 2Ô∏è‚É£ Exploration initiale des donn√©es\n",
     "V√©rification de la structure des fichiers (types de variables, valeurs manquantes, distributions)."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "print('--- Info SIRH ---')\n",
     "sirh.info()\n",
     "print('--- Info Eval ---')\n",
     "evals.info()\n",
     "print('--- Info Sondage ---')\n",
     "sondage.info()"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 3Ô∏è‚É£ Nettoyage et harmonisation\n",
     "- Normalisation des noms de colonnes\n",
     "- Cr√©ation d‚Äôun identifiant commun (`id_employee`)\n",
     "- Alignement des trois fichiers"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "sirh.columns = sirh.columns.str.lower().str.strip()\n",
     "evals.columns = evals.columns.str.lower().str.strip()\n",
     "sondage.columns = sondage.columns.str.lower().str.strip()\n",
     "\n",
     "evals['id_employee'] = evals['eval_number'].str.replace('E_', '').astype(int)\n",
     "sondage = sondage.rename(columns={'code_sondage': 'id_employee'})"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 4Ô∏è‚É£ Construction du DataFrame central\n",
     "Jointure des diff√©rentes sources pour obtenir une base unique."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "df = pd.merge(sirh, evals, on='id_employee', how='inner')\n",
     "df = pd.merge(df, sondage, on='id_employee', how='inner')\n",
     "display(df.head())"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 5Ô∏è‚É£ Statistiques descriptives\n",
     "Premi√®res observations selon le statut de d√©part des employ√©s."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "display(df.groupby('a_quitte_l_entreprise').mean(numeric_only=True))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 6Ô∏è‚É£ Visualisations exploratoires\n",
     "Exemple : relation entre le revenu, le poste et le d√©part des collaborateurs."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "plt.figure(figsize=(6,4))\n",
     "sns.boxplot(x='a_quitte_l_entreprise', y='revenu_mensuel', data=df)\n",
     "plt.title(\"Revenu mensuel vs D√©part de l'entreprise\")\n",
     "plt.show()\n",
     "\n",
     "plt.figure(figsize=(10,4))\n",
     "sns.countplot(x='poste', hue='a_quitte_l_entreprise', data=df)\n",
     "plt.title(\"Poste vs D√©part de l'entreprise\")\n",
     "plt.xticks(rotation=45)\n",
     "plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 7Ô∏è‚É£ Pr√©paration des donn√©es pour la mod√©lisation\n",
     "- S√©paration features/target\n",
     "- Encodage des variables cat√©gorielles"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "X = df.drop(columns=['a_quitte_l_entreprise', 'id_employee'])\n",
     "y = df['a_quitte_l_entreprise']\n",
     "\n",
     "def encode_categorical_features(df, ordinal_cols=None, nominal_cols=None):\n",
     "    df_encoded = df.copy()\n",
     "    if ordinal_cols:\n",
     "        for col, categories in ordinal_cols.items():\n",
     "            enc = OrdinalEncoder(categories=[categories])\n",
     "            df_encoded[col] = enc.fit_transform(df[[col]])\n",
     "    if nominal_cols:\n",
     "        enc = OneHotEncoder(drop='first', sparse_output=False)\n",
     "        nominal_encoded = enc.fit_transform(df[nominal_cols])\n",
     "        nominal_encoded_df = pd.DataFrame(nominal_encoded, columns=enc.get_feature_names_out(nominal_cols), index=df.index)\n",
     "        df_encoded = pd.concat([df_encoded.drop(columns=nominal_cols), nominal_encoded_df], axis=1)\n",
     "    return df_encoded\n",
     "\n",
     "ordinal_mapping = {'niveau_education': [1,2,3,4,5]}\n",
     "nominal_cols = ['genre', 'departement', 'poste', 'domaine_etude', 'ayant_enfants', 'frequence_deplacement', 'heure_supplementaires']\n",
     "X_encoded = encode_categorical_features(X, ordinal_cols=ordinal_mapping, nominal_cols=nominal_cols)\n",
     "display(X_encoded.head())"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 8Ô∏è‚É£ Analyse des corr√©lations\n",
     "V√©rification des relations entre variables num√©riques et avec la cible."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "numeric_cols = X_encoded.select_dtypes(exclude=['object']).columns\n",
     "\n",
     "plt.figure(figsize=(10,8))\n",
     "sns.heatmap(X_encoded[numeric_cols].corr(), annot=False, cmap='coolwarm')\n",
     "plt.title('Corr√©lations de Pearson')\n",
     "plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 9Ô∏è‚É£ Mod√©lisation\n",
     "Nous comparons trois mod√®les :\n",
     "- **DummyClassifier** (baseline)\n",
     "- **R√©gression Logistique**\n",
     "- **RandomForestClassifier**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
     "\n",
     "models = {\n",
     "    'Dummy': DummyClassifier(strategy='most_frequent'),\n",
     "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
     "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
     "}\n",
     "\n",
     "for name, model in models.items():\n",
     "    print(f'=== Mod√®le : {name} ===')\n",
     "    model.fit(X_train, y_train)\n",
     "    print('Test Metrics:')\n",
     "    y_test_pred = model.predict(X_test)\n",
     "    print(classification_report(y_test, y_test_pred, zero_division=0))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üîü √âvaluation avanc√©e (RandomForest)\n",
     "- Matrice de confusion\n",
     "- Importance des variables"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "best_model = models['RandomForest']\n",
     "y_test_pred = best_model.predict(X_test)\n",
     "\n",
     "# Matrice de confusion\n",
     "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, cmap=\"Blues\")\n",
     "plt.title(\"Matrice de confusion - RandomForest\")\n",
     "plt.show()\n",
     "\n",
     "# Importance des variables\n",
     "importances = best_model.feature_importances_\n",
     "indices = np.argsort(importances)[::-1][:10]\n",
     "\n",
     "plt.figure(figsize=(8,5))\n",
     "sns.barplot(x=importances[indices], y=X_train.columns[indices])\n",
     "plt.title(\"Top 10 variables importantes (RandomForest)\")\n",
     "plt.show()"
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.13"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
 